---
title: "Introduction"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

As the number of cyber-attacks continues to grow on a daily basis, so has the delay in threat detection. For instance, in 2015, the Office of Personnel Management (OPM) discovered that approximately [21.5 million individual records of Federal employees and contractors had been stolen](https://www.opm.gov/cybersecurity/cybersecurity-incidents). On average, the time between an attack and its discovery is [more than 200 days](https://www.wired.com/2016/10/inside-cyberattack-shocked-us-government/). In the case of the OPM breach, the attack had been going on for almost a year. Currently, cyber analysts inspect numerous potential incidents on a daily basis, but have neither the time nor the resources available to perform such a task.  `anomalyDetection` aims to curtail the time frame in which cyber-attacks go unnoticed and to aid in the discovery of these attacks among the millions of daily logged events, while minimizing the number of false positives and negatives. By incorporating a tabular vector approach along with multivariate analysis functionality, `anomalyDetection` provides cyber analysts the ability to effectively and efficiently identify time periods associated with suspected anomalies for further evaluation.


## Functions

`anomalyDetection` provides 13 functions to aid in the detection of potential cyber anomalies:

| Function | Purpose |
|:-----------|:--------------------------------------------------------|
| `tabulated_state_vector` | Employs a tabulated vector approach to transform security log data into unique counts of data attributes based on time blocks.    
| `block_inspect`          | Creates a list where the original data has been divided into blocks denoted in the state vector.    
| `mc_adjust`              | Handles issues with multi-collinearity.    
| `mahalanobis_distance`   | Calculates the distance between the elements in data and the mean vector of the data for outlier detection.
| `bd_row`                 | Indicates which variables in data are driving the Mahalanobis distance for a specific row, relative to the mean vector of the data.    
| `horns_curve`            | Computes [Horn's Parallel Analysis](https://link.springer.com/article/10.1007/BF02289447) to determine the factors to retain within a factor analysis.
| `factor_analysis`        | Reduces the structure of the data by relating the correlation between variables to a set of factors, using the eigen-decomposition of the correlation matrix.
| `factor_analysis_results`| Provides easy access to factor analysis results.
| `kaisers_index`          | Computes scores designed to assess the quality of a factor analysis solution. It measures the tendency towards unifactoriality for both a given row and the entire matrix as a whole.
| `principal_components`   | Relates the data to a set of a components through the eigen-decomposition of the correlation matrix, where each component explains some variance of the data.
| `principal_components_results`  | Provides easy access to principal component analysis results.
| `get_all_factors`        | finds all factor pairs for a given integer.

`anomalyDetection` also incorporates the pipe operator (`%>%`) from the [magrittr package]() for streamlining function composition.  To illustrate the functionality of `anomalyDetection` we will use the `security_logs` data that comes with the package and mimics common information that appears in security logs.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
security_logs <- anomalyDetection::security_logs
security_logs <- tibble::as_tibble(security_logs)
```

```{r, collapse=TRUE, message=FALSE, warning=FALSE}
# we'll use tidyverse for some common manipulations
library(tidyverse)
library(anomalyDetection)

security_logs
```


## State Vector Creation

To apply the statistical methods that we'll see in the sections that follow, we employ the tabulated vector approach. This approach transforms the security log data into unique counts of data attributes based on pre-defined time blocks.  Therefore, as each time block is generated, the categorical fields are separated by their levels and a count of occurrences for each level are recorded into a vector. All numerical fields, such as bytes in and bytes out, are recorded as a summation within the time block. The result is what we call a "state vector matrix".

Thus, for our `security_logs` data we can create our state vector matrix based on our data being divided into 10 time blocks.  What results is the summary of instances for each categorical level in our data for each time block.  Consequently, row one represents the first time block and there were 2 instances of CISCO as the device vendor, 1 instances of IBM, etc.

```{r, collapse=TRUE}
tabulate_state_vector(security_logs, 10)
```



## Multicollinearity Adjustment

Prior to proceeding with any multivariate statistical analyses we should inspect the state vector for multicollinearity, to avoid issues such as matrix singularity, rank deficiency, and strong correlation values, and remove any columns that pose an issue. We can use `mc_adjust` to handle issues with multi-collinearity by first removing any columns whose variance is close to or less than a minimum level of variance (`min_var`). Then, it removes linearly dependent columns. Finally, it removes any columns that have a high absolute correlation value equal to or greater than `max_cor`.  

```{r, collapse=TRUE}
(state_vec <- security_logs %>%
  tabulate_state_vector(10) %>%
  mc_adjust())
```

By default, `mc_adjust` removes any columns that violate the variance, dependency, and correlation thresholds.  Alternatively, we can use `action = "select"` as an argument, which provides interactivity where the user can select the variables that violate the correlation threshold that they would like to remove.


## Multivariate Statistical Analyses

With our data adjusted for multicollinearity we can now proceed with multivariate analyses.  First we'll use the `mahalanobis_distance` function to compute the distance between the elements in the data and the mean vector of the data for outlier detection. Here, we include `output = "both"` to return both the Mahalanobis distance and the absolute breakdown distances.

```{r, collapse=TRUE}
state_vec %>%
  mahalanobis_distance("both", normalize = TRUE) %>%
  as_tibble
```

We can use this information in a modified heatmap visualization to identify outlier values across our categorical variables and time blocks.  The larger and brighter the dot the larger the outlier is and deserves attention.

```{r, fig.align='center', fig.width=9, fig.height=6}
state_vec %>%
  mahalanobis_distance("both", normalize = TRUE) %>%
  as_tibble %>%
  mutate(Block = 1:n()) %>% 
  gather(Variable, BD, -c(MD, Block)) %>%
  ggplot(aes(factor(Block), Variable, color = MD, size = BD)) +
  geom_point()
```


We can build onto this with the `bd_row` to identify which variables in the data are driving the Mahalanobis distance.  `bd_row` will look at a specified row and rank-order the columns by those that are driving the Mahalanobis distance.  For example, the plot above identified block 17 as having the largest Mahalanobis distance suggesting some abnormal activity may be occuring during that time block.  We can drill down into that block and look at the top 10 variables that are driving the Mahalanobis distance as these may be areas that require further investigation.

```{r, collapse=TRUE}
state_vec %>%
  mahalanobis_distance("bd", normalize = TRUE) %>%
  bd_row(17, 10)
```



